{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_Zh35KXy-22"
   },
   "source": [
    "##### 1. Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "executionInfo": {
     "elapsed": 1137,
     "status": "ok",
     "timestamp": 1757737733267,
     "user": {
      "displayName": "Srilakshmi M",
      "userId": "08083791182778891935"
     },
     "user_tz": -720
    },
    "id": "Jq83VJUYy_pY",
    "outputId": "766eaf3e-ec00-45e4-e428-090818b3bfd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset shape: (4999, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4999,\n  \"fields\": [\n    {\n      \"column\": \"What are the special things we (husband and me) can do during a 5 day stay at Cape Town?\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4992,\n        \"samples\": [\n          \"Which is the best option for buffet at Surfers Paradise Seafood Buffet at Citrique Marriott or Four Winds Crown?\",\n          \"Any suggestions of what to see in El Paso?\",\n          \"What are the restaurants near Trastevere station?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TTD\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"TTD\\n\",\n          \"TGU\",\n          \"FOD\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TTDSIG\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 79,\n        \"samples\": [\n          \"FODBAK\",\n          \"TTDOTH\",\n          \"FODAUT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-e3958e71-a596-469e-9be8-8cbf607038fb\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>What are the special things we (husband and me) can do during a 5 day stay at Cape Town?</th>\n",
       "      <th>TTD</th>\n",
       "      <th>TTDSIG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the companies which organize shark fe...</td>\n",
       "      <td>TTD</td>\n",
       "      <td>TTDOTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is it safe for female traveller to go alone to...</td>\n",
       "      <td>TGU</td>\n",
       "      <td>TGUHEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the best places around Cape Town for ...</td>\n",
       "      <td>TTD</td>\n",
       "      <td>TTDSIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the best places to stay for a family ...</td>\n",
       "      <td>ACM</td>\n",
       "      <td>ACMOTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the train services that travels from ...</td>\n",
       "      <td>TRS</td>\n",
       "      <td>TRSTRN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3958e71-a596-469e-9be8-8cbf607038fb')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-e3958e71-a596-469e-9be8-8cbf607038fb button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-e3958e71-a596-469e-9be8-8cbf607038fb');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-b6c5aca7-1b2c-4f83-8007-83327fb7d5d2\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b6c5aca7-1b2c-4f83-8007-83327fb7d5d2')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-b6c5aca7-1b2c-4f83-8007-83327fb7d5d2 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "  What are the special things we (husband and me) can do during a 5 day stay at Cape Town?  \\\n",
       "0  What are the companies which organize shark fe...                                         \n",
       "1  Is it safe for female traveller to go alone to...                                         \n",
       "2  What are the best places around Cape Town for ...                                         \n",
       "3  What are the best places to stay for a family ...                                         \n",
       "4  What are the train services that travels from ...                                         \n",
       "\n",
       "   TTD  TTDSIG  \n",
       "0  TTD  TTDOTH  \n",
       "1  TGU  TGUHEA  \n",
       "2  TTD  TTDSIG  \n",
       "3  ACM  ACMOTH  \n",
       "4  TRS  TRSTRN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, os, io, requests\n",
    "ASSIGNMENT2_CSV = \"https://raw.githubusercontent.com/suralk/travel_domain_data/master/5000TravelQuestionsDataset.csv\"\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "def fetch_csv(url, path):\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "csv_path = \"data/travel-classification.csv\"\n",
    "if not os.path.exists(csv_path):\n",
    "    fetch_csv(ASSIGNMENT2_CSV, csv_path)\n",
    "df = pd.read_csv(csv_path,encoding=\"latin-1\")\n",
    "print(\"\\nDataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqVbdckgzCJL"
   },
   "source": [
    "###### 1.1 Data cleaning and sanity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1757737742322,
     "user": {
      "displayName": "Srilakshmi M",
      "userId": "08083791182778891935"
     },
     "user_tz": -720
    },
    "id": "Fk_r_d7AzIB1",
    "outputId": "a707cb38-c22c-4ffd-9ccd-9744a72136f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After cleaning:\n",
      "Shape: (4992, 2)\n",
      "Any nulls?\n",
      " question        0\n",
      "coarse_label    0\n",
      "dtype: int64\n",
      "\n",
      "Coarse label counts:\n",
      "coarse_label\n",
      "TGU      1216\n",
      "TTD      1137\n",
      "TRS      1011\n",
      "ACM       717\n",
      "FOD       521\n",
      "ENT       214\n",
      "WTH       170\n",
      "TGU\\n       3\n",
      "\\nENT       2\n",
      "TTD\\n       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "All expected coarse classes present!\n",
      "\n",
      "Question length stats:\n",
      "count    4992.000000\n",
      "mean       60.470553\n",
      "std        20.360721\n",
      "min        17.000000\n",
      "25%        46.000000\n",
      "50%        57.000000\n",
      "75%        70.000000\n",
      "max       181.000000\n",
      "Name: q_length, dtype: float64\n",
      "Shortest question example: Do we need shoes?\n",
      "Longest question example: Can anyone tell me if the spa at the Hilton Moorea is massages only or if there is an area with jacuzzi's and any other relaxing things like that that is for the spa customers only?\n",
      "\n",
      "Unique labels after normalization: ['TTD' 'TGU' 'ACM' 'TRS' 'WTH' 'FOD' 'ENT']\n"
     ]
    }
   ],
   "source": [
    "# Renaming columns for clarity\n",
    "df.columns = [\"question\", \"coarse_label\", \"fine_label\"]\n",
    "# Droping the fine-grain column\n",
    "df = df.drop(columns=[\"fine_label\"])\n",
    "# Removing duplicates and nulls\n",
    "df = df.drop_duplicates(subset=\"question\")\n",
    "df = df.dropna()\n",
    "# Strip whitespace and normalize spaces\n",
    "df[\"question\"] = df[\"question\"].str.strip()\n",
    "df[\"question\"] = df[\"question\"].str.replace(r'\\s+', ' ', regex=True)\n",
    "# Sanity checks\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Any nulls?\\n\", df.isnull().sum())\n",
    "# Coarse label distribution\n",
    "print(\"\\nCoarse label counts:\")\n",
    "print(df[\"coarse_label\"].value_counts())\n",
    "# Check for expected classes\n",
    "expected_classes = {\"TTD\",\"TGU\",\"ACM\",\"TRS\",\"WTH\",\"FOD\",\"ENT\"}\n",
    "missing_classes = expected_classes - set(df[\"coarse_label\"].unique())\n",
    "if missing_classes:\n",
    "    print(\"\\nWarning: Missing expected classes ->\", missing_classes)\n",
    "else:\n",
    "    print(\"\\nAll expected coarse classes present!\")\n",
    "# Question length sanity\n",
    "df[\"q_length\"] = df[\"question\"].str.len()\n",
    "print(\"\\nQuestion length stats:\")\n",
    "print(df[\"q_length\"].describe())\n",
    "print(\"Shortest question example:\", df.loc[df[\"q_length\"].idxmin(), \"question\"])\n",
    "print(\"Longest question example:\", df.loc[df[\"q_length\"].idxmax(), \"question\"])\n",
    "# Normalize coarse labels: strip whitespace and newlines, uppercase\n",
    "df[\"coarse_label\"] = df[\"coarse_label\"].str.strip().str.upper()\n",
    "# Verify again\n",
    "print(\"\\nUnique labels after normalization:\", df[\"coarse_label\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbZoyx4tzKKG"
   },
   "source": [
    "##### 2. Train/validation/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 873,
     "status": "ok",
     "timestamp": 1757737749959,
     "user": {
      "displayName": "Srilakshmi M",
      "userId": "08083791182778891935"
     },
     "user_tz": -720
    },
    "id": "Jw6oqbjlzO4Y",
    "outputId": "74d7cc56-da08-4e72-e40d-d9e488751de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (4000, 3)\n",
      "Validation set shape: (292, 3)\n",
      "Test set shape: (700, 3)\n",
      "\n",
      "Training label distribution:\n",
      " coarse_label\n",
      "TGU    967\n",
      "TTD    914\n",
      "TRS    820\n",
      "ACM    565\n",
      "FOD    423\n",
      "ENT    172\n",
      "WTH    139\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation label distribution:\n",
      " coarse_label\n",
      "TTD    66\n",
      "TGU    64\n",
      "TRS    60\n",
      "ACM    51\n",
      "FOD    31\n",
      "WTH    11\n",
      "ENT     9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label distribution:\n",
      " coarse_label\n",
      "TGU    188\n",
      "TTD    158\n",
      "TRS    131\n",
      "ACM    101\n",
      "FOD     67\n",
      "ENT     35\n",
      "WTH     20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting the data into 700 for test and the rest for training/validation\n",
    "train_val_df, test_df = train_test_split(df, test_size=700, random_state=42, shuffle=True)\n",
    "\n",
    "# splitting the remaining data (train_val_df) into 4000 for training and 300 for validation\n",
    "train_df, val_df = train_test_split(train_val_df, train_size=4000, random_state=42, shuffle=True)\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Training set shape:\", train_df.shape)  # Should be (4000, X)\n",
    "print(\"Validation set shape:\", val_df.shape)  # Should be (300, X)\n",
    "print(\"Test set shape:\", test_df.shape)      # Should be (700, X)\n",
    "\n",
    "# Checking label distribution\n",
    "print(\"\\nTraining label distribution:\\n\", train_df['coarse_label'].value_counts())\n",
    "print(\"\\nValidation label distribution:\\n\", val_df['coarse_label'].value_counts())\n",
    "print(\"\\nTest label distribution:\\n\", test_df['coarse_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb3oLkTDzR0w"
   },
   "source": [
    "##### Installing unsloth beore model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EW2eDa8DzVfw"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install unsloth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKQodnRAzX78"
   },
   "source": [
    "##### 3. Model Selection : The selected model is a 4-bit quantized  version of LLaMA-3.2B-Instruct from Unsloth\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kUDJ7r9l6M6"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lUQs_AugsP6"
   },
   "source": [
    "##### 4. Supervised Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 897692,
     "status": "ok",
     "timestamp": 1757743551834,
     "user": {
      "displayName": "Srilakshmi M",
      "userId": "08083791182778891935"
     },
     "user_tz": -720
    },
    "id": "tupDsZFElbxU",
    "outputId": "e4a9987c-2cc3-4f92-d6ae-438d479001b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2669255515.py:53: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing configuration: LR=5e-06, Batch Size=4, Epochs=1 ===\n",
      "==((====))==  Unsloth 2025.9.4: Fast Llama patching. Transformers: 4.56.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipython-input-2669255515.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "Training: 100%|██████████| 1000/1000 [13:10<00:00,  1.26it/s]\n",
      "/tmp/ipython-input-2669255515.py:130: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9.8529\n",
      "Validation Loss: 2.3970\n",
      ">>> New best model found!\n",
      "\n",
      "=== Best Config: {'learning_rate': 5e-06, 'batch_size': 4, 'epochs': 1} ===\n",
      "Best Validation Loss: 2.3970\n",
      "\n",
      "Loading best model for final test evaluation...\n",
      "==((====))==  Unsloth 2025.9.4: Fast Llama patching. Transformers: 4.56.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTesting:   0%|          | 0/175 [00:00<?, ?it/s]/tmp/ipython-input-2669255515.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "Testing: 100%|██████████| 175/175 [00:53<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Loss: 2.4379\n",
      "Final Test Accuracy: 0.8069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Dataset class\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        prompt = f\"Q: {row['question']}\\nA:\"\n",
    "        completion = row['coarse_label']\n",
    "        enc = self.tokenizer(\n",
    "            prompt,\n",
    "            text_target=completion,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"labels\": enc[\"labels\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# Tokenizer & Model Config\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "\n",
    "# Datasets\n",
    "train_dataset = QADataset(train_df, tokenizer)\n",
    "val_dataset = QADataset(val_df, tokenizer)\n",
    "test_dataset = QADataset(test_df, tokenizer)\n",
    "\n",
    "# Hyperparameter grid\n",
    "learning_rates = [5e-6]\n",
    "batch_sizes = [4]\n",
    "epochs_list = [3]\n",
    "\n",
    "# Tracking best config\n",
    "best_val_loss = float(\"inf\")\n",
    "best_config = None\n",
    "best_model_path = \"best_model\"\n",
    "\n",
    "\n",
    "# Grid Search\n",
    "for lr, batch_size, epochs in itertools.product(learning_rates, batch_sizes, epochs_list):\n",
    "    print(f\"\\n=== Testing configuration: LR={lr}, Batch Size={batch_size}, Epochs={epochs} ===\")\n",
    "\n",
    "    # Load base model fresh\n",
    "    model, _ = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=1024,\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Apply LoRA\n",
    "    lora_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.1)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        print(f\"Train Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        print(\">>> New best model found!\")\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_config = {\"learning_rate\": lr, \"batch_size\": batch_size, \"epochs\": epochs}\n",
    "        if os.path.exists(best_model_path):\n",
    "            shutil.rmtree(best_model_path)\n",
    "        model.save_pretrained(best_model_path)\n",
    "        tokenizer.save_pretrained(best_model_path)\n",
    "\n",
    "    # Clean up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Final Evaluation\n",
    "print(f\"\\n=== Best Config: {best_config} ===\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nLoading best model for final test evaluation...\")\n",
    "\n",
    "# Load base + LoRA model\n",
    "# Load the quantized base model\n",
    "base_model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=1024,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,  # Automatically sets to float16 for 4bit models\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Attach the LoRA adapters\n",
    "best_model = PeftModel.from_pretrained(base_model, best_model_path)\n",
    "best_model.eval()\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_config[\"batch_size\"], pin_memory=True)\n",
    "\n",
    "test_loss = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            outputs = best_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        mask = labels != -100\n",
    "\n",
    "        masked_preds = preds[mask]\n",
    "        masked_labels = labels[mask]\n",
    "\n",
    "        all_preds.extend(masked_preds.cpu().numpy())\n",
    "        all_labels.extend(masked_labels.cpu().numpy())\n",
    "\n",
    "# Final metrics\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"\\nFinal Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Final Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGpJU2q3zhKS"
   },
   "source": [
    "##### 5. Prompt Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7JZW7vbziW1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from unsloth import FastLanguageModel\n",
    "import pandas as pd\n",
    "import difflib\n",
    "\n",
    "# Loading best_model and tokenizer\n",
    "base_model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"  # original base model repo\n",
    "lora_model_path = \"best_model\"  # fine-tuned LoRA checkpoint folder\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name=base_model_name,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_path)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def create_prompt(question, examples=None):\n",
    "    \"\"\"\n",
    "    examples: list of tuples [(q1, a1), ...] for few-shot prompting\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Your task is to classify the following question into one of these categories: \"\n",
    "        \"TTD, TGU, ACM, TRS, WTH, FOD, ENT. \"\n",
    "    )\n",
    "\n",
    "    if examples:\n",
    "        prompt += \"Here are some examples:\\n\"\n",
    "        for q, a in examples:\n",
    "            prompt += f\"Q: {q}\\nA: {a}\\n\\n\"\n",
    "\n",
    "    prompt += f\"Now classify this question:\\nQ: {question}\\nA:\"\n",
    "    return prompt\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_capability = torch.cuda.get_device_capability()\n",
    "    if device_capability[0] < 8:  # compute capability < 8.0\n",
    "        os.environ[\"XFORMERS_NO_MEM_EFF_ATTENTION\"] = \"1\"\n",
    "        print(f\"Memory-efficient attention disabled (GPU capability {device_capability})\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#predict function\n",
    "def predict(question, model, tokenizer, examples_list=None, max_length=10, temperature=0.01, top_p=0.99):\n",
    "    prompt = create_prompt(question, examples_list)\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids, attention_mask = enc[\"input_ids\"], enc[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "\n",
    "    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    answer = answer.replace(prompt, \"\").strip()\n",
    "    return map_to_valid_label(answer)\n",
    "\n",
    "# --- Evaluation function ---\n",
    "def evaluate(test_questions, test_labels, model, tokenizer, examples_list=None, temperature=0.01, top_p=0.99):\n",
    "    preds = []\n",
    "    for q in tqdm(test_questions, desc=\"Evaluation\"):\n",
    "        pred = predict(\n",
    "            q,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            examples_list=examples_list,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        preds.append(pred)\n",
    "\n",
    "    true_labels_norm = [normalize_label(l) for l in test_labels]\n",
    "    acc = accuracy_score(true_labels_norm, preds)\n",
    "    return acc, preds\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6mSAXdKzlsE"
   },
   "source": [
    "##### 6. Zero-shot and few-shot testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7fkCqjezk2M"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#  Normalizing labels\n",
    "def normalize_label(label):\n",
    "    return label.strip().upper()\n",
    "\n",
    "valid_labels = {\"TTD\",\"TGU\",\"ACM\",\"TRS\",\"WTH\",\"FOD\",\"ENT\"}\n",
    "\n",
    "\n",
    "\n",
    "# Map prediction safely\n",
    "def map_to_valid_label(pred):\n",
    "    pred_norm = normalize_label(pred)\n",
    "    for label in valid_labels:\n",
    "        if label in pred_norm:   # substring match\n",
    "            return label\n",
    "    # fallback: picking closest match instead of UNKNOWN\n",
    "    return max(valid_labels, key=lambda x: difflib.SequenceMatcher(None, pred_norm, x).ratio())\n",
    "\n",
    "# Zero shot\n",
    "zero_shot_acc, pred_labels_zero = evaluate(\n",
    "    test_df['question'], test_df['coarse_label'], model, tokenizer,\n",
    "    examples_list=None, temperature=0.01, top_p=0.9\n",
    ")\n",
    "print(\"Zero-shot accuracy:\", zero_shot_acc)\n",
    "\n",
    "# 1-shot\n",
    "example_1 = train_df.sample(1, random_state=42)\n",
    "examples_1 = list(zip(example_1['question'], example_1['coarse_label']))\n",
    "\n",
    "acc_1shot, pred_labels_1shot = evaluate(\n",
    "    test_df['question'], test_df['coarse_label'], model, tokenizer,\n",
    "    examples_list=examples_1, temperature=0.01, top_p=0.9\n",
    ")\n",
    "print(\"1-shot accuracy:\", acc_1shot)\n",
    "\n",
    "# 3-shot\n",
    "example_3 = train_df.sample(3, random_state=42)\n",
    "examples_3 = list(zip(example_3['question'], example_3['coarse_label']))\n",
    "\n",
    "acc_3shot, pred_labels_3shot = evaluate(\n",
    "    test_df['question'], test_df['coarse_label'], model, tokenizer,\n",
    "    examples_list=examples_3, temperature=0.01, top_p=0.9\n",
    ")\n",
    "print(\"3-shot accuracy:\", acc_3shot)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
